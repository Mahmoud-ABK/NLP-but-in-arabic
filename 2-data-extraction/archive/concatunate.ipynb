{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c20336a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headers for 02-extracted-pages.csv:\n",
      "['article_id', 'title', 'title_en', 'abstract_ar', 'abstract_en', 'general_field', 'field', 'authors', 'authors_en', 'publish_date', 'source', 'page1', 'page2']\n",
      "\n",
      "Headers for 2-step2.csv:\n",
      "['article_id', 'title', 'title_en', 'abstract_ar', 'abstract_en', 'general_field', 'field', 'authors', 'authors_en', 'publish_date', 'source', 'page1', 'page2', 'page3', 'page1_token_count', 'page2_token_count', 'page3_token_count']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths to your CSV files\n",
    "file1 = \"02-extracted-pages.csv\"\n",
    "file2 = \"2-step2.csv\"\n",
    "\n",
    "def print_headers(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, nrows=0)  # load only header row\n",
    "        print(f\"\\nHeaders for {path}:\")\n",
    "        print(list(df.columns))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "\n",
    "print_headers(file1)\n",
    "print_headers(file2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc42a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! New file created: 02-extracted-pages-with-tokens.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the two CSVs\n",
    "df_main = pd.read_csv(\"02-extracted-pages.csv\")\n",
    "df_tokens = pd.read_csv(\"2-step2.csv\")\n",
    "\n",
    "# Select only the needed columns\n",
    "df_tokens_small = df_tokens[[\n",
    "    \"article_id\",\n",
    "    \"page1_token_count\",\n",
    "    \"page2_token_count\"\n",
    "]]\n",
    "\n",
    "# Merge on article_id\n",
    "df_merged = df_main.merge(df_tokens_small, on=\"article_id\", how=\"left\")\n",
    "\n",
    "# Save the updated CSV\n",
    "df_merged.to_csv(\"02-extracted-pages-with-tokens.csv\", index=False)\n",
    "\n",
    "print(\"Done! New file created: 02-extracted-pages-with-tokens.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cb35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id                     0\n",
      "title                       1587\n",
      "title_en                    1443\n",
      "abstract_ar                 1635\n",
      "abstract_en                 1635\n",
      "general_field                189\n",
      "field                       1635\n",
      "authors                     1429\n",
      "authors_en                  1635\n",
      "publish_date                1635\n",
      "source                         0\n",
      "page1                          0\n",
      "page2                          0\n",
      "page1_token_count              0\n",
      "page2_token_count              0\n"
     ]
    }
   ],
   "source": [
    "for col in df_merged.columns:\n",
    "    print(f\"{col:<20}  {df_merged[col].isnull().sum():>10}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07bcc6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries missing title_en and title: 1429\n"
     ]
    }
   ],
   "source": [
    "count_missing = df_merged[df_merged[\"title\"].isnull() & df_merged[\"title_en\"].isnull()].shape[0]\n",
    "print(\"entries missing title_en and title:\", count_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0588cd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in general_field:\n",
      "['Art' 'Edu' 'Bus' 'Agricultural' 'Arabic' 'Business' 'Education']\n",
      "\n",
      "Unique values in field:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values in general_field:\")\n",
    "print(df_merged[\"general_field\"].dropna().unique())\n",
    "\n",
    "print(\"\\nUnique values in field:\")\n",
    "print(df_merged[\"field\"].dropna().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6d95796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AJP' 'AJSP' 'ajsrp' 'AM' 'ARPD']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"02-extracted-pages-with-tokens.csv\")\n",
    "print(df[\"source\"].dropna().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41a8df",
   "metadata": {},
   "source": [
    "# extract field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8770e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pip install langchain langchain-openai pydantic\n",
    "\n",
    "Server:\n",
    "  llama.cpp running at http://localhost:6000/v1\n",
    "  model alias: qwen-extractor-1\n",
    "\"\"\"\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List, Any, Dict\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc391d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sample = \"\"\"\n",
    "   Association of Arab Universities Journal for Education and\n",
    "\n",
    "Psychology\n",
    "Volume 23 | Issue 2 Article 1\n",
    "2025\n",
    "\n",
    "لمهارات البحث الإجرائي وصعوبات تنفيذه من خلال إعدادهم\n",
    "‎The Level of Acquisition of Action Research‏ .لمشروع التخرجح\n",
    "‎Skills and Difficulties in its preperarions Through Graduation‏\n",
    "‎Project Among College of Education Students at Sultan Qaboos‏\n",
    "‎University‏\n",
    "\n",
    "نور النجار\n",
    "‎nour.najar@seciauni.org‏ ,كلية التربية,. جامعة السلطان ‎pools‏ عمان\n",
    "\n",
    "Follow this and additional works at: https://digitalcommons.aaru.edu.jo/aaru_jep\n",
    "\n",
    "Digibalrt of the Education Commons\n",
    "Commons\n",
    "\n",
    "Network ‏ا‎\n",
    "\n",
    "Recommended Citation\n",
    "\n",
    "النجار, نور )2025( \"درجة اكتساب طلبة كلية ‎av yall‏ بجامعة السلطان قابوس لمهارات البحث الإجرائي وص 992ب\n",
    "\n",
    "‎The Level of Acquisition of Action Research Skills and Difficulties in‏ .تنفيذه من خلال إعدادهم لمشروع التخرج\n",
    "‎its preperarions Through Graduation Project Among College of Education Students at Sultan Qaboos‏\n",
    "‎University,’ Association of Arab Universities Journal for Education and Psychology. Vol. 23: Iss. 2, Article‏\n",
    ".1\n",
    "\n",
    "‎Available at: https://digitalcommons.aaru.edu.jo/aaru_jep/vol23/iss2/1\n",
    "\n",
    "‎This Article is brought to you for free and open access by Arab Journals Platform. It has been accepted for\n",
    "inclusion in Association of Arab Universities Journal for Education and Psychology by an authorized editor. The\n",
    "journal is hosted on Digital Commons, an Elsevier platform. For more information, please contact\n",
    "marah@aaru.edu.jo,rakan@aaru.edu.jo .\n",
    "\n",
    "مجلة اتحاد الجامعات العربية للتربية وعلم النفس 0 0 ...ممم المجلد الثالث والعشرون - العدد الثاني - 8٠؟5١؟‏\n",
    "\n",
    "‎day‏ اكتساب طلبة ‎dS‏ التربية بجامعة السلطان قابوس لمهارات البحث\n",
    "الإجرائي وصعوبات تنفيذه من خلال إعدادهم لمشروع التخرج\n",
    "\n",
    "‏د. نور النجار ”\n",
    "الملخص\n",
    "هدفت هذه الدراسة إلى الكشف عن درحة اكتساب طلبة كلية التربية بجامعة السلطان قابوس\n",
    "لمهارات البحث الإحرائي» وصعوبات تنفيذه من خلال تطبيقهم لمشروع التخرج» استخدم للجمع البيانات\n",
    "أداتان: الأولى معايير التقييم لمشروع التخرج المعدة من قبل كلية التربية بجامعة السلطان قابوس» وتكونت\n",
    "العينة لحذه الأداة من ‎UG Wh Kee‏ التربية بيجامعة السلطان قابوس» والأداة الثانية كانت عبارة\n",
    "مقابلات ‎cre Ved‏ وقد أظهرت نتائج الاستبانة ‎of‏ اكتساب طلبة ‎als‏ التربية لمهارات البحث\n",
    "الإحرائي جاءت جميعها بمستوى مرتفع عدا مهارة توثيق المراجع» وعلى الرغم من وحود درحات مرتفعة\n",
    "لاكتسابهم تلك المهارات إلا أن نتائج المقابلات كشفت عن عدد من الصعوبات التي واحهتهم في تنفيذ\n",
    "مشروع التخرج من أهمها: تيار الموضوع؛ وصياغة الأسئلة البحثية» وتطبيق الأدوات» وتحليل البيانات»\n",
    "وقلة المصادر والدراسات السابقة» وكتابة البحثء كما أظهرت النتائج عدم وحود فروق ‎ONS‏ ذلالة\n",
    "إحصائية بين متوسطات الذكور والإناث في اكتساب مهارات البحث الإحرائي» ‎lay‏ أظهرت النتائج\n",
    "وجود فروق ذات دلالة إحصائية ترجع لنوعية البرنامج (بكالوريوس» ودبلوم تأهيل تربوي) لصالح برنامج\n",
    "دبلوم التأهيل التربوي. ‎By‏ ضوء تلك النتائج توصي الباحثتان بتدريب طلبة مؤسسات التعليم العاللي على\n",
    "القيام بالبحث الإجرائي في عدد من المقررات وعدم اقتصاره على مقرر واحد ‎badd‏ وتقدهم ورش تدريبية\n",
    "لمدرسي مقرر مشروع التخرج عن أفضل الممارسات التدريسية في تدريس البحث الإجرائي.\n",
    "الكلمات المفتاحية: كلية التربية» البحث الإجرائي» جامعة السلطان قابوس» الصعوبات» مشروع\n",
    "‎cel‏\n",
    "\n",
    "‏* أستاذ مساعد, قسم المناهج والتدريس» كلية التربية» جامعة السلطان قابوس» عمان.\n",
    "\n",
    "‎١\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dca6e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ajsp = \"\"\"\n",
    "المجلة العببية ‎[pi‏\n",
    "\n",
    "الإصدار السابع — العدد ستة وستون\n",
    "تاريخ الإصدار: 2 - نيسان - 2024م\n",
    "\n",
    "www.ajsp.net\n",
    "\n",
    "ISSN: 2663-5798 || Arab Journal for Scientific Publishing\n",
    "\n",
    "INTE” Gam By Few ‏سم حك ون سد بسب‎ GSD _\n",
    "\n",
    "\"المشكلات السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس\n",
    "من وجهة نظر المعلمين\"\n",
    "\n",
    "إعداد الباحثة:\n",
    "\n",
    "أ. نفيسة سعيدة\n",
    "الجامعة العربية الأمربكية\n",
    "\n",
    "Arab Journal for Scientific Publishing (AJSP) ISSN: 2663-5798\n",
    "\n",
    "دتمل\n",
    "\n",
    "الإصدار السابع — العدد ستة وستون\n",
    "تاريخ الإصدار: 2 - نيسان - 2024م\n",
    "\n",
    "www.ajsp.net\n",
    "\n",
    "ISSN: 2663-5798 || Arab Journal for Scientific Publishing\n",
    "\n",
    "IN! TRNAS Foe oe tty Aes ge SB ‏نود‎ CO) _ ©\n",
    "\n",
    "الملخص:\n",
    "\n",
    "هدفت الدراسة التعرف إلى المشكلات السلوكية ‎Gal‏ طلبة ذوي صعويات التعلم المحددة في المدارس الاساسية داخل القدس من\n",
    "‎Ages‏ نظر المعلمين» والتعرف إلى دور المتغيرات الجنس» وسنوات الخبرة» والمؤهل العلمي في استجابات المعلمين نحو المشكلات\n",
    "السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس» وقد تكون مجتمع الدراسة من جميع معلمي\n",
    "المدارس الأساسية في مدينة القدس والبالغ عددهم (1761)» وتكونت عينة الدراسة من (119) معلماً ومعلمة اختيرت بالطريقة الغرضية\n",
    "الهادفة» ولتحقيق أهداف الدراسة استخدمت الباحثة المنهج الوصفي بصورته التحليلية » واستخدام الاستبانة كأداة للدراسة» ويعد تحليل\n",
    "النتائج باستخدام برنامج الرزم الإحصائية للعلوم الاجتماعية ‎(SPSS)‏ أظهرت النتائج أن المتوسطات الحسابية والانحرافات المعيارية\n",
    "للمشكلات السلوكية لدى طلبة ذوي صعويات التعلم المحددة في المدارس الاساسية داخل القدس من ‎Ages‏ نظر المعلمين تراوحت ما\n",
    "بين (3.61-2.88) إذ كانت النسبة المئوبة للاستجابة لها تتراوح ما ‎cas‏ )%57.6-%72.2(« فقد أتى المجال الأول السلوك المتعلق\n",
    "بالنشاط الزائد في الترتيب الأول ويمتوسط حسابي مقداره (3.61) ونسبة مئوية )%72.2( وهي درجة كبيرة ‎Lele‏ في المرتبة الثانية\n",
    "المتعلق بالسلوك العدواني ‎cla‏ المجال الرابع وبمتوسط حسابي مقداره )3.35( ونسبة مئوية )%67-0( » وقد أتى المجال الثاني السلوك\n",
    "الاجتماعي المنحرف في المرتبة الثالثة ويمتوسط حسابي (3.14) وبنسبة مئوية )%62.8( ‎٠‏ في ‎Gus‏ جاء المجال الثالث والمتعلق\n",
    "بالعادات الغريبة واللزمات العصبية في المرتبة الأخيرة وبمتوسط حسابي )2-88( ونسبة مئوية )%57.6( ‎Lele‏ الدرجة الكلية فقد حصلت\n",
    "على متوسط حسابي )3.26( ونسبة مئوية (7665.2) وهي درجة متوسطة» ‎aly‏ تظهر فروق دالة إحصائياً في استجابات المعلمين نحو\n",
    "المشكلات السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس تعزى لمتغيري الجنس والمؤهل العلمي»\n",
    "‎Ley‏ ظهرت فروق في متغير سنوات الخدمة ولصالح أكثر من )10( سنوات وأوصت الدراسة بتفعيل دور الإرشاد التربوي في المدارس\n",
    "الأساسية في مدينة القدس في تنفيذ أنشطة إرشادية من شأنها تقليل حدة النشاط الزائد لدى الطلبة من ذوي صعويات التعلم المحددة؛\n",
    "وعقد دورات تدريبية للمعلمين الجدد حول مهارات التعامل مع المشكلات الصفية ومنها المشكلات السلوكية لذوي صعويات التعلم\n",
    "المحددة.\n",
    "\n",
    "الكلمات المفتاحية: المشكلات السلوكية؛ صعويات التعلم المحددة» المدارس الأساسية.\n",
    "\n",
    "6.\n",
    "\n",
    "المقدمه:\n",
    "\n",
    "تعتبر صعويات التعلم المحددة مهمة في الوقت الحاضر في مجال التعليم العادي والتعليم الخاص على حد سواء»ء وتلقى اهتماما\n",
    "كبيرا من مختلف مجالات العلوم سواء الطبية, النفسية» والتربودة الاجتماعية, وهذا الاهتمام طبيعى» حيث تشكل هذه المجموعة شريحة\n",
    "تشمل جميع ‎OU‏ التعليم الخاص.\n",
    "\n",
    "ونتيجة لتزايد ظهور الصعويات التعليمية المحددة ‎(cal‏ الأطفال في الآونة ‎Bud)‏ حرص بعض التريوبين والأخصائيين على ‎ESI‏\n",
    "‏عن ‎Ghul‏ الكامنة وراء هذه المشاكل أو الصعوباتء؛ واكتشاف العوامل والآثار التي تؤدي بشكل مباشر أو غير مباشر إلى هذه\n",
    "\n",
    "الصعوية. فقد مر مفهوم صعويات التعلم المحددة بتطور من ‎Gus‏ المصطلح وذلك نتيجة الجهود المستمرة في دراسته» وانتشر واستخدم\n",
    "في المجالات التعليمية» وقد ظهر مصطلح صعويات التعلم المحددة نتيجة لما لاحظه اختصاصيو التوعية في الخمسينات والستينات»\n",
    "\n",
    "Arab Journal for Scientific Publishing (AJSP) ISSN: 2663-5798   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729150bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text_for_llm(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # 1) Unicode normalization (VERY important)\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    # 2) Remove invisible / directional characters (category Cf)\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch) != \"Cf\")\n",
    "\n",
    "    # 3) Remove decorative punctuation & symbols\n",
    "    text = re.sub(\n",
    "        r\"[•●▪♦■★☆※…—–―‐-⁃⁄⁎⁑⁂⁕⁖⁗⁘⁙⁚⁛⁜⁝⁞]\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # 4) Normalize repeated punctuation\n",
    "    text = re.sub(r\"\\.{3,}\", \".\", text)\n",
    "    text = re.sub(r\"[=]{2,}\", \"\", text)\n",
    "    text = re.sub(r\"[_~^`]+\", \"\", text)\n",
    "\n",
    "    # 5) Normalize quotes\n",
    "    text = re.sub(r\"[“”]\", '\"', text)\n",
    "    text = re.sub(r\"[‘’]\", \"'\", text)\n",
    "\n",
    "    # 6) Normalize whitespace\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7d5d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt 1 : 4-5 min\n",
    "class GeneralField(str, Enum):\n",
    "    ART = \"Art\"\n",
    "    AGRICULTURAL = \"Agricultural\"\n",
    "    ARABIC = \"Arabic\"\n",
    "    BUSINESS = \"Business\"\n",
    "    EDUCATION = \"Education\"\n",
    "    Engineering = \"Engineering\"\n",
    "    MEDICAL = \"Medical\"\n",
    "    MUSIC = \"Music\"\n",
    "    SCIENCE = \"Science\"\n",
    "    SOCIAL_SCIENCE = \"Social Science\"\n",
    "    COMPUTER_SCIENCE = \"Computer Science\"\n",
    "    HISTORY = \"History\"\n",
    "    PSYCHOLOGY = \"Psychology\"\n",
    "\n",
    "\n",
    "\n",
    "class ArticleMetadata(BaseModel):\n",
    "    # Non-optional: always filled\n",
    "    title_ar: str = Field(..., description=\"Arabic title (extracted or translated).\")\n",
    "    title_en: str = Field(..., description=\"English title (extracted or translated).\")\n",
    "    abstract_ar: str = Field(..., description=\"Arabic abstract (extracted or translated).\")\n",
    "    abstract_en: str = Field(..., description=\"English abstract (extracted or translated).\")\n",
    "\n",
    "    # Non-optional: must be one of enum values\n",
    "    general_field: GeneralField = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"One of: Art, Agricultural, Arabic, Business, Education, \"\n",
    "            \"Engineering, Medical, Music, Science, Social Science, \"\n",
    "            \"Computer Science, History , Psychology.\"\n",
    "        ),\n",
    "    )\n",
    "    # Authors can be empty list if not found (still non-optional)\n",
    "    authors: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of author names (in original script, Arabic or Latin).\",\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:6000/v1\",\n",
    "    api_key=\"no-key\",\n",
    "    model=\"qwen-extractor-1\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=ArticleMetadata)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "You are an information extraction + normalization model.\n",
    "\n",
    "Input text is mostly Arabic with some English mixed in.\n",
    "\n",
    "You MUST output ALL fields in the schema (no nulls, no missing keys).\n",
    "\n",
    "Extraction rules:\n",
    "1) If title_ar is present in the text, copy it exactly.\n",
    "   If title_ar is NOT present but title_en is present, TRANSLATE title_en to Arabic and put it in title_ar.\n",
    "   If neither title is present, GENERATE a short informative title in BOTH languages based on the text.\n",
    "\n",
    "2) If title_en is present, copy it exactly.\n",
    "   If not present but title_ar is present, TRANSLATE title_ar to English.\n",
    "   If neither is present, generate both (as above).\n",
    "\n",
    "3) For abstract_ar / abstract_en:\n",
    "   - If present, copy exactly.\n",
    "   - If missing in one language but present in the other, TRANSLATE.\n",
    "   - If neither abstract is present, write a concise abstract (5-10 sentences) in BOTH languages based on the text.\n",
    "\n",
    "4) authors:\n",
    "   - Extract author names if explicitly present \n",
    "   - Keep names exactly as written (Arabic or Latin).\n",
    "   - If not found, return [].\n",
    "\n",
    "5) general_field:\n",
    "    - Classify the article into EXACTLY ONE of the following values: Art, Agricultural, Arabic, Business, Education, Engineering, Medical, Music, Science, Social Science, Computer Science, History, Psychology.\n",
    "    - Choose the closest dominant field and never invent new labels.\n",
    "\n",
    "IMPORTANT:\n",
    "- Output MUST follow the format instructions exactly.\n",
    "- Return only valid JSON, no extra text.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "        ),\n",
    "        (\"human\", 'Text:\\n\"\"\"\\n{text}\\n\"\"\"'),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "\n",
    "def extract_article_metadata(text: str) -> ArticleMetadata:\n",
    "    text = clean_text_for_llm(text)\n",
    "    result: Dict[str, Any] = chain.invoke({\"text\": text})\n",
    "    return ArticleMetadata(**result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e403ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt 2 : 4-5 min\n",
    "class GeneralField(str, Enum):\n",
    "    ART = \"Art\"\n",
    "    AGRICULTURAL = \"Agricultural\"\n",
    "    ARABIC = \"Arabic\"\n",
    "    BUSINESS = \"Business\"\n",
    "    EDUCATION = \"Education\"\n",
    "    Engineering = \"Engineering\"\n",
    "    MEDICAL = \"Medical\"\n",
    "    MUSIC = \"Music\"\n",
    "    SCIENCE = \"Science\"\n",
    "    SOCIAL_SCIENCE = \"Social Science\"\n",
    "    COMPUTER_SCIENCE = \"Computer Science\"\n",
    "    HISTORY = \"History\"\n",
    "    PSYCHOLOGY = \"Psychology\"\n",
    "\n",
    "\n",
    "\n",
    "class ArticleMetadata(BaseModel):\n",
    "    # Non-optional: always filled\n",
    "    title_ar: str = Field(..., description=\"Arabic title (extracted or translated).\")\n",
    "    title_en: str = Field(..., description=\"English title (extracted or translated).\")\n",
    "    abstract_ar: str = Field(..., description=\"Arabic abstract (extracted or translated).\")\n",
    "    abstract_en: str = Field(..., description=\"English abstract (extracted or translated).\")\n",
    "\n",
    "    # Non-optional: must be one of enum values\n",
    "    general_field: GeneralField = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"One of: Art, Agricultural, Arabic, Business, Education, \"\n",
    "            \"Engineering, Medical, Music, Science, Social Science, \"\n",
    "            \"Computer Science, History , Psychology.\"\n",
    "        ),\n",
    "    )\n",
    "    # Authors can be empty list if not found (still non-optional)\n",
    "    authors: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of author names (in original script, Arabic or Latin).\",\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:6000/v1\",\n",
    "    api_key=\"no-key\",\n",
    "    model=\"qwen-extractor-1\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=ArticleMetadata)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "You are an information extraction + normalization model.\n",
    "Input text is mostly Arabic with some English mixed in.\n",
    "You MUST output ALL fields in the schema (no nulls, no missing keys).\n",
    "IMPORTANT:\n",
    "- Output MUST follow the format instructions exactly.\n",
    "- Return only valid JSON, no extra text.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\",\n",
    "        ),\n",
    "        (\"human\", 'Text:\\n\"\"\"\\n{text}\\n\"\"\"'),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "\n",
    "def extract_article_metadata(text: str) -> ArticleMetadata:\n",
    "    text = clean_text_for_llm(text)\n",
    "    result: Dict[str, Any] = chain.invoke({\"text\": text})\n",
    "    return ArticleMetadata(**result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "370ac668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic prompt token counting: 619\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    resp = requests.post(\n",
    "        \"http://localhost:6000/tokenize\",\n",
    "        json={\"content\": text},\n",
    "        timeout=10,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    return len(data[\"tokens\"])\n",
    "\n",
    "print (\"basic prompt token counting:\" ,    count_tokens(prompt.format_prompt(text=\"\").to_string())  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "519170c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt count_cleaned: 1640\n",
      "Prompt count_uncleaned: 1716\n",
      "Difference in tokens due to cleaning: 76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example\n",
    "raw_text = sample  # Replace with your actual text\n",
    "clean_text = clean_text_for_llm(raw_text)\n",
    "messages = prompt.format_messages(text=clean_text)\n",
    "full_prompt_cleaned = \"\\n\\n\".join(m.content for m in messages)\n",
    "full_tokens = count_tokens(full_prompt_cleaned)\n",
    "print(\"Prompt count_cleaned:\", full_tokens)\n",
    "\n",
    "\n",
    "raw_text = sample  # Replace with your actual text\n",
    "messages = prompt.format_messages(text=raw_text)\n",
    "full_prompt = \"\\n\\n\".join(m.content for m in messages)\n",
    "full_tokens_not_cleaned = count_tokens(full_prompt)\n",
    "print(\"Prompt count_uncleaned:\", full_tokens_not_cleaned)\n",
    "\n",
    "print (\"Difference in tokens due to cleaning:\", full_tokens_not_cleaned - full_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbc630b",
   "metadata": {},
   "source": [
    "# pinpointing sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56d2d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Callable\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Core utilities (shared)\n",
    "# =========================\n",
    "\n",
    "def _normalize_unicode_and_strip_invisibles(text: str) -> str:\n",
    "    \"\"\"NFKC normalize + remove invisible formatting chars (category Cf).\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = \"\".join(ch for ch in text if unicodedata.category(ch) != \"Cf\")\n",
    "    return text\n",
    "\n",
    "def _basic_cleanup(text: str) -> str:\n",
    "    \"\"\"Light OCR cleanup that is safe for Arabic + English.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = _normalize_unicode_and_strip_invisibles(text)\n",
    "\n",
    "    # Remove decorative punctuation/symbols\n",
    "    text = re.sub(r\"[•●▪♦■★☆※…—–―‐-⁃⁄⁎⁑⁂⁕⁖⁗⁘⁙⁚⁛⁜⁝⁞]\", \" \", text)\n",
    "\n",
    "    # Normalize repeated punctuation\n",
    "    text = re.sub(r\"\\.{3,}\", \".\", text)\n",
    "    text = re.sub(r\"[=]{2,}\", \" \", text)\n",
    "    text = re.sub(r\"[_~^`]+\", \" \", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def _join_pages(page1: str, page2: str) -> str:\n",
    "    p1 = _basic_cleanup(page1 or \"\")\n",
    "    p2 = _basic_cleanup(page2 or \"\")\n",
    "    if p1 and p2:\n",
    "        return p1 + \"\\n\\n===== PAGE 2 =====\\n\\n\" + p2\n",
    "    return p1 or p2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9641352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# -------------------------\n",
    "# Author cues (fallback)\n",
    "# -------------------------\n",
    "AR_AUTHOR_CUES = [\"جامعة\", \"كلية\", \"قسم\", \"أستاذ\", \"د.\", \"دكتور\", \"المؤلف\", \"المؤلفون\", \"باحث\"]\n",
    "EN_AUTHOR_CUES = [\"university\", \"college\", \"department\", \"author\", \"authors\", \"prof\", \"dr.\"]\n",
    "\n",
    "# -------------------------\n",
    "# Abstract / keywords cues\n",
    "# Keep only header-like cues here (high precision).\n",
    "# \"هدفت/نتائج/منهج...\" are content cues; they can cause false positives as anchors.\n",
    "# -------------------------\n",
    "ABSTRACT_HDR_AR = [\"الملخص\", \"ملخص\", \"الخلاصة\", \"مستخلص\"]\n",
    "ABSTRACT_HDR_EN = [\"abstract\", \"summary\"]\n",
    "\n",
    "KEYWORDS_HDR_AR = [\"الكلمات المفتاحية\", \"كلمات مفتاحية\", \"الكلمات الدالة\", \"كلمات دالة\"]\n",
    "KEYWORDS_HDR_EN = [\"keywords\", \"key words\", \"index terms\"]\n",
    "\n",
    "# -------------------------\n",
    "# Compiled regexes (compile once)\n",
    "# -------------------------\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "\n",
    "# For safety with English word boundaries\n",
    "ABSTRACT_RX = re.compile(r\"\\babstract\\b\", re.IGNORECASE)\n",
    "KEYWORDS_RX = re.compile(r\"\\bkeywords?\\b\", re.IGNORECASE)\n",
    "\n",
    "# AJP boilerplate patterns (compile once)\n",
    "AJP_BLOCK_PATTERNS = [\n",
    "    re.compile(r\"Follow this and additional works at:.*?Recommended Citation\", re.IGNORECASE | re.DOTALL),\n",
    "    re.compile(r\"Recommended Citation.*?Available at:\\s*https?://\\S+\", re.IGNORECASE | re.DOTALL),\n",
    "    re.compile(r\"This Article is brought to you.*?(?:\\n\\s*\\n|$)\", re.IGNORECASE | re.DOTALL),\n",
    "    re.compile(r\"It has been accepted for.*?(?:\\n\\s*\\n|$)\", re.IGNORECASE | re.DOTALL),\n",
    "]\n",
    "\n",
    "AJP_LINE_PATTERNS = [\n",
    "    re.compile(r\"^Association of Arab Universities Journal for Education and\\s*Psychology\\s*$\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^Volume\\s+\\d+\\s*\\|\\s*Issue\\s+\\d+\\s*Article\\s+\\d+\\s*$\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^\\s*\\d{4}\\s*$\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^مجلة اتحاد الجامعات العربية للتربية وعلم النفس.*$\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^.*المجلد.*العدد.*$\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^\\s*[0-9\\u0660-\\u0669\\u06F0-\\u06F9]+\\s*$\", re.IGNORECASE | re.MULTILINE),\n",
    "    re.compile(r\"^\\s*Available at:\\s*https?://\\S+\\s*$\", re.IGNORECASE | re.MULTILINE),\n",
    "]\n",
    "\n",
    "RUNNING_HEADER_RX = re.compile(r\"^[^\\n]{20,200}(?:\\.\\.\\.|؛).*$\", re.MULTILINE)\n",
    "\n",
    "\n",
    "def ajp_strip_boilerplate(clean_text: str) -> str:\n",
    "    \"\"\"Assumes text is already cleaned/normalized by your _basic_cleanup/_join_pages pipeline.\"\"\"\n",
    "    text = clean_text\n",
    "\n",
    "    for rx in AJP_BLOCK_PATTERNS:\n",
    "        text = rx.sub(\"\", text)\n",
    "\n",
    "    for rx in AJP_LINE_PATTERNS:\n",
    "        text = rx.sub(\"\", text)\n",
    "\n",
    "    # remove common running header line on page 2\n",
    "    text = RUNNING_HEADER_RX.sub(\"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def _prep_cues(*lists: List[str]) -> List[str]:\n",
    "    \"\"\"Lowercase + remove duplicates while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for L in lists:\n",
    "        for x in L:\n",
    "            k = x.lower()\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                out.append(k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _slice_lines(lines: List[str], start: int, end: int) -> str:\n",
    "    start = max(0, start)\n",
    "    end = min(len(lines), end)\n",
    "    if start >= end:\n",
    "        return \"\"\n",
    "    return \"\\n\".join(lines[start:end]).strip()\n",
    "\n",
    "\n",
    "def _region_from_indices(lines: List[str], idxs: List[int], radius: int = 2) -> str:\n",
    "    if not idxs:\n",
    "        return \"\"\n",
    "    keep = set()\n",
    "    for i in idxs:\n",
    "        for j in range(max(0, i - radius), min(len(lines), i + radius + 1)):\n",
    "            keep.add(j)\n",
    "    return \"\\n\".join(lines[i] for i in sorted(keep)).strip()\n",
    "\n",
    "\n",
    "def AJP_pinpoint_imp(page1: str, page2: str, max_chars: int = 9000) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic AJP pinpointer (optimized):\n",
    "    - join+clean (via _join_pages)\n",
    "    - strip AJP boilerplate\n",
    "    - single pass over lines to detect anchors (abstract/keywords/emails/author-cues)\n",
    "    - deterministic slicing into regions\n",
    "    \"\"\"\n",
    "    text = _join_pages(page1, page2)\n",
    "    text = ajp_strip_boilerplate(text)\n",
    "\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "\n",
    "    # Prepare cues once\n",
    "    abs_cues = _prep_cues(ABSTRACT_HDR_AR, ABSTRACT_HDR_EN)\n",
    "    key_cues = _prep_cues(KEYWORDS_HDR_AR, KEYWORDS_HDR_EN)\n",
    "    author_cues = _prep_cues(AR_AUTHOR_CUES, EN_AUTHOR_CUES)\n",
    "\n",
    "    abs_idx: Optional[int] = None\n",
    "    key_idx: Optional[int] = None\n",
    "    email_idxs: List[int] = []\n",
    "    authorcue_idxs: List[int] = []\n",
    "\n",
    "    # ---- Single pass ----\n",
    "    for i, ln in enumerate(lines):\n",
    "        ln_low = ln.lower()\n",
    "\n",
    "        # abstract header\n",
    "        if abs_idx is None:\n",
    "            if any(ln_low.startswith(c) for c in abs_cues) or ABSTRACT_RX.search(ln):\n",
    "                abs_idx = i\n",
    "\n",
    "        # keywords header\n",
    "        if key_idx is None:\n",
    "            if any(ln_low.startswith(c) for c in key_cues) or KEYWORDS_RX.search(ln):\n",
    "                key_idx = i\n",
    "\n",
    "        # emails\n",
    "        if EMAIL_RE.search(ln):\n",
    "            email_idxs.append(i)\n",
    "\n",
    "        # author cues (fallback)\n",
    "        # note: we keep this lightweight; no regex needed\n",
    "        if any(c in ln_low for c in author_cues):\n",
    "            authorcue_idxs.append(i)\n",
    "\n",
    "    # ABSTRACT_REGION\n",
    "    abstract_region = \"\"\n",
    "    if abs_idx is not None:\n",
    "        end = key_idx if (key_idx is not None and key_idx > abs_idx) else len(lines)\n",
    "        abstract_region = _slice_lines(lines, abs_idx, end)\n",
    "\n",
    "    # KEYWORDS_REGION (keywords line + next line)\n",
    "    keywords_region = \"\"\n",
    "    if key_idx is not None:\n",
    "        keywords_region = _slice_lines(lines, key_idx, min(len(lines), key_idx + 2))\n",
    "\n",
    "    # AUTHORS_REGION (emails first, else author cues)\n",
    "    if email_idxs:\n",
    "        authors_region = _region_from_indices(lines, email_idxs, radius=2)\n",
    "    else:\n",
    "        # Author cues can be noisy; tighten:\n",
    "        # Only consider cues in the first ~40 lines (typical metadata zone)\n",
    "        authorcue_idxs_top = [i for i in authorcue_idxs if i <= 40]\n",
    "        authors_region = _region_from_indices(lines, authorcue_idxs_top, radius=1)\n",
    "\n",
    "    # TITLE_REGION (top until earliest of email/abstract; else top 25)\n",
    "    cut_candidates = []\n",
    "    if email_idxs:\n",
    "        cut_candidates.append(email_idxs[0])  # already in ascending order\n",
    "    if abs_idx is not None:\n",
    "        cut_candidates.append(abs_idx)\n",
    "    cutoff = min(cut_candidates) if cut_candidates else min(len(lines), 25)\n",
    "\n",
    "    title_region_lines = lines[:cutoff]\n",
    "    title_region_lines = [ln for ln in title_region_lines if not URL_RE.search(ln)]\n",
    "    title_region = \"\\n\".join(title_region_lines[:30]).strip()\n",
    "\n",
    "    llm_text = (\n",
    "        \"TITLE_REGION:\\n\" + title_region +\n",
    "        \"\\n\\nAUTHORS_REGION:\\n\" + authors_region +\n",
    "        \"\\n\\nABSTRACT_REGION:\\n\" + abstract_region +\n",
    "        \"\\n\\nKEYWORDS_REGION:\\n\" + keywords_region\n",
    "    ).strip()\n",
    "\n",
    "    return llm_text[:max_chars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17e34056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajsp optimized pinpointer test\n",
    "import re\n",
    "from typing import List, Pattern, Optional\n",
    "\n",
    "# ============================================================\n",
    "# 1) AJSP boilerplate patterns (KEEP AS STRINGS + COMPILE)\n",
    "# ============================================================\n",
    "\n",
    "AJSP_BLOCK_PATTERNS: List[str] = [\n",
    "    # repeated journal footer/header blocks\n",
    "    r\"Arab Journal for Scientific Publishing\\s*\\(AJSP\\)\\s*ISSN:\\s*2663-5798.*?(?:\\n\\s*\\n|$)\",\n",
    "]\n",
    "\n",
    "AJSP_LINE_PATTERNS: List[str] = [\n",
    "    r\"^المجلة\\s+الع(?:ب|ر)بية\\s+للنشر.*$\",\n",
    "    r\"^ISSN:\\s*2663-5798\\s*\\|\\|\\s*Arab Journal for Scientific Publishing.*$\",\n",
    "    r\"^Arab Journal for Scientific Publishing\\s*\\(AJSP\\)\\s*ISSN:\\s*2663-5798.*$\",\n",
    "    r\"^www\\.ajsp\\.net\\s*$\",\n",
    "    r\"^https?://doi\\.org/\\S+\\s*$\",\n",
    "    r\"^الإصدار.*$\",\n",
    "    r\"^العدد.*$\",\n",
    "    r\"^تاريخ الإصدار.*$\",\n",
    "    r\"^\\s*[0-9\\u0660-\\u0669\\u06F0-\\u06F9]+\\s*\\.?\\s*$\",  # page numbers like 13, 54, 6.\n",
    "]\n",
    "\n",
    "def compile_patterns(patterns: List[str], flags: int) -> List[Pattern]:\n",
    "    return [re.compile(p, flags) for p in patterns]\n",
    "\n",
    "AJSP_BLOCK_RX: List[Pattern] = compile_patterns(AJSP_BLOCK_PATTERNS, re.IGNORECASE | re.DOTALL)\n",
    "AJSP_LINE_RX: List[Pattern]  = compile_patterns(AJSP_LINE_PATTERNS,  re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "def ajsp_strip_boilerplate(clean_text: str) -> str:\n",
    "    \"\"\"Assumes text already normalized by _join_pages / _basic_cleanup pipeline.\"\"\"\n",
    "    text = clean_text\n",
    "    for rx in AJSP_BLOCK_RX:\n",
    "        text = rx.sub(\"\", text)\n",
    "    for rx in AJSP_LINE_RX:\n",
    "        text = rx.sub(\"\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) CUES + REGEXES\n",
    "# ============================================================\n",
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
    "URL_RE   = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "\n",
    "# Abstract/keywords headers (high precision)\n",
    "ABSTRACT_HDR_AR = [\"الملخص\", \"ملخص\", \"الخلاصة\", \"مستخلص\"]\n",
    "ABSTRACT_HDR_EN = [\"abstract\"]\n",
    "\n",
    "KEYWORDS_HDR_AR = [\"الكلمات المفتاحية\", \"كلمات مفتاحية\", \"الكلمات الدالة\", \"كلمات دالة\"]\n",
    "KEYWORDS_HDR_EN = [\"keywords\", \"key words\", \"index terms\"]\n",
    "\n",
    "# Intro headers (stop markers)\n",
    "INTRO_HDR_AR = [\"المقدمة\", \"مقدمة\"]\n",
    "INTRO_HDR_EN = [\"introduction\"]\n",
    "\n",
    "# Strong author markers (AJSP-specific)\n",
    "AUTHOR_MARKERS_AR = [\"إعداد الباحثة\", \"إعداد الباحث\", \"إعداد\", \"الباحثة\", \"الباحث\", \"الكاتبة\", \"الكاتب\"]\n",
    "AUTHOR_MARKERS_EN = [\"researchers\", \"researcher\", \"author\", \"authors\"]\n",
    "\n",
    "# Fallback author cues (lower precision)\n",
    "AR_AUTHOR_CUES = [\"جامعة\", \"كلية\", \"قسم\", \"أستاذ\", \"دكتور\", \"المؤلف\", \"المؤلفون\", \"باحث\"]\n",
    "EN_AUTHOR_CUES = [\"university\", \"college\", \"department\", \"faculty\", \"prof\"]\n",
    "\n",
    "\n",
    "def _prep_cues(*lists: List[str]) -> List[str]:\n",
    "    \"\"\"Lowercase + remove duplicates while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    out: List[str] = []\n",
    "    for L in lists:\n",
    "        for x in L:\n",
    "            k = x.lower()\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                out.append(k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _slice_lines(lines: List[str], start: int, end: int) -> str:\n",
    "    start = max(0, start)\n",
    "    end = min(len(lines), end)\n",
    "    if start >= end:\n",
    "        return \"\"\n",
    "    return \"\\n\".join(lines[start:end]).strip()\n",
    "\n",
    "\n",
    "def _region_from_indices(lines: List[str], idxs: List[int], radius: int = 2) -> str:\n",
    "    if not idxs:\n",
    "        return \"\"\n",
    "    keep = set()\n",
    "    for i in idxs:\n",
    "        for j in range(max(0, i - radius), min(len(lines), i + radius + 1)):\n",
    "            keep.add(j)\n",
    "    return \"\\n\".join(lines[i] for i in sorted(keep)).strip()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) AJSP PINPOINTER (DETERMINISTIC)\n",
    "# ============================================================\n",
    "\n",
    "def AJSP_pinpoint_imp(page1: str, page2: str, max_chars: int = 9000) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic AJSP pinpointer:\n",
    "      - join+clean (via _join_pages)\n",
    "      - strip AJSP boilerplate\n",
    "      - single pass over lines to detect anchors (abstract/keywords/intro/emails/author-markers)\n",
    "      - slice into labeled regions for the LLM\n",
    "    \"\"\"\n",
    "    text = _join_pages(page1, page2)\n",
    "    text = ajsp_strip_boilerplate(text)\n",
    "\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "\n",
    "    abs_cues    = _prep_cues(ABSTRACT_HDR_AR, ABSTRACT_HDR_EN)\n",
    "    key_cues    = _prep_cues(KEYWORDS_HDR_AR, KEYWORDS_HDR_EN)\n",
    "    intro_cues  = _prep_cues(INTRO_HDR_AR, INTRO_HDR_EN)\n",
    "    author_mark = _prep_cues(AUTHOR_MARKERS_AR, AUTHOR_MARKERS_EN)\n",
    "    author_cues = _prep_cues(AR_AUTHOR_CUES, EN_AUTHOR_CUES)\n",
    "\n",
    "    abs_idx: Optional[int] = None\n",
    "    key_idx: Optional[int] = None\n",
    "    intro_idx: Optional[int] = None\n",
    "\n",
    "    email_idxs: List[int] = []\n",
    "    author_marker_idxs: List[int] = []\n",
    "    authorcue_idxs: List[int] = []\n",
    "\n",
    "    # ---- Single pass ----\n",
    "    for i, ln in enumerate(lines):\n",
    "        lo = ln.lower()\n",
    "\n",
    "        if abs_idx is None and any(lo.startswith(c) for c in abs_cues):\n",
    "            abs_idx = i\n",
    "        if key_idx is None and any(lo.startswith(c) for c in key_cues):\n",
    "            key_idx = i\n",
    "        if intro_idx is None and any(lo.startswith(c) for c in intro_cues):\n",
    "            intro_idx = i\n",
    "\n",
    "        if EMAIL_RE.search(ln):\n",
    "            email_idxs.append(i)\n",
    "\n",
    "        # author marker lines (higher precision)\n",
    "        if any(m in lo for m in author_mark):\n",
    "            author_marker_idxs.append(i)\n",
    "\n",
    "        # author cue fallback (lower precision)\n",
    "        if any(c in lo for c in author_cues):\n",
    "            authorcue_idxs.append(i)\n",
    "\n",
    "    # ABSTRACT_REGION: end priority = keywords > intro > hard cap\n",
    "    abstract_region = \"\"\n",
    "    if abs_idx is not None:\n",
    "        if key_idx is not None and key_idx > abs_idx:\n",
    "            end = key_idx\n",
    "        elif intro_idx is not None and intro_idx > abs_idx:\n",
    "            end = intro_idx\n",
    "        else:\n",
    "            end = min(len(lines), abs_idx + 60)  # cap if missing markers\n",
    "        abstract_region = _slice_lines(lines, abs_idx, end)\n",
    "\n",
    "    # KEYWORDS_REGION (keywords line + next line)\n",
    "    keywords_region = \"\"\n",
    "    if key_idx is not None:\n",
    "        keywords_region = _slice_lines(lines, key_idx, min(len(lines), key_idx + 2))\n",
    "\n",
    "    # AUTHORS_REGION: emails > explicit markers > cues in top metadata area\n",
    "    if email_idxs:\n",
    "        authors_region = _region_from_indices(lines, email_idxs, radius=2)\n",
    "    elif author_marker_idxs:\n",
    "        start = author_marker_idxs[0]\n",
    "        authors_region = _slice_lines(lines, start, min(len(lines), start + 10))\n",
    "    else:\n",
    "        top = [i for i in authorcue_idxs if i <= 50]\n",
    "        authors_region = _region_from_indices(lines, top, radius=1)\n",
    "\n",
    "    # TITLE_REGION: stop at earliest of author marker, email, abstract, doi\n",
    "    doi_idxs = [i for i, ln in enumerate(lines) if \"doi.org\" in ln.lower()]\n",
    "    cut_candidates: List[int] = []\n",
    "    if author_marker_idxs:\n",
    "        cut_candidates.append(author_marker_idxs[0])\n",
    "    if email_idxs:\n",
    "        cut_candidates.append(email_idxs[0])\n",
    "    if abs_idx is not None:\n",
    "        cut_candidates.append(abs_idx)\n",
    "    if doi_idxs:\n",
    "        cut_candidates.append(doi_idxs[0])\n",
    "\n",
    "    cutoff = min(cut_candidates) if cut_candidates else min(len(lines), 25)\n",
    "\n",
    "    title_region_lines = lines[:cutoff]\n",
    "    title_region_lines = [ln for ln in title_region_lines if not URL_RE.search(ln)]\n",
    "    title_region = \"\\n\".join(title_region_lines[:35]).strip()\n",
    "\n",
    "    llm_text = (\n",
    "        \"TITLE_REGION:\\n\" + title_region +\n",
    "        \"\\n\\nAUTHORS_REGION:\\n\" + authors_region +\n",
    "        \"\\n\\nABSTRACT_REGION:\\n\" + abstract_region +\n",
    "        \"\\n\\nKEYWORDS_REGION:\\n\" + keywords_region\n",
    "    ).strip()\n",
    "\n",
    "    return llm_text[:max_chars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2c475a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJSRP pinpointer + cleaning\n",
    "import re\n",
    "from typing import List, Pattern, Optional\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) AJSRP boilerplate patterns (KEEP AS STRINGS + COMPILE)\n",
    "# ============================================================\n",
    "\n",
    "AJSRP_BLOCK_PATTERNS: List[str] = [\n",
    "    # Open access / license block (often appears as a footer chunk)\n",
    "    r\"This article is an open\\s*access article distributed.*?(?:\\n\\s*\\n|$)\",\n",
    "    r\"under the terms and\\s*conditions of the Creative Commons.*?(?:\\n\\s*\\n|$)\",\n",
    "    # Publisher rights block\n",
    "    r\"\\d{4}\\s*©\\s*AISRP.*?all\\s*rights\\s*reserved\\.(?:\\n\\s*\\n|$)\",\n",
    "]\n",
    "\n",
    "AJSRP_LINE_PATTERNS: List[str] = [\n",
    "    # Journal header lines\n",
    "    r\"^Arab Journal of Sciences\\s*&\\s*Research Publishing\\s*\\(AJSRP\\).*$\",\n",
    "    r\"^https?://journals\\.ajsrp\\.com/\\S*\\s*$\",\n",
    "    r\"^ISSN:\\s*2518-5780\\s*\\(Online\\).*$\",\n",
    "    r\"^ISSN:\\s*2518-5780\\s*\\(Print\\).*$\",\n",
    "    r\"^ISSN:\\s*2518-5780.*$\",\n",
    "\n",
    "    # Pagination line like \"Vol 11, Issue 3 (2025) ٠ P: 69 - 4\"\n",
    "    r\"^.*Vol\\s*\\d+,\\s*Issue\\s*\\d+\\s*\\(\\d{4}\\)\\s*.*P:\\s*\\d+.*$\",\n",
    "\n",
    "    # Dates / metadata labels (keep content in regions if you want, but usually noise)\n",
    "    r\"^\\s*Received:\\s*$\",\n",
    "    r\"^\\s*Revised:\\s*$\",\n",
    "    r\"^\\s*Accepted:\\s*$\",\n",
    "    r\"^\\s*Published:\\s*$\",\n",
    "    r\"^\\s*\\*?\\s*Corresponding author:\\s*$\",\n",
    "    r\"^\\s*Citation:\\s*.*$\",\n",
    "\n",
    "    # Standalone page numbers like 54, 38, 74, etc.\n",
    "    r\"^\\s*[0-9\\u0660-\\u0669\\u06F0-\\u06F9]+\\s*\\.?\\s*$\",\n",
    "\n",
    "    # Open access markers\n",
    "    r\"^\\s*°\\s*Open Access\\s*$\",\n",
    "    r\"^\\s*\\*\\s*NCO\\s*ND\\s*$\",\n",
    "]\n",
    "\n",
    "def compile_patterns(patterns: List[str], flags: int) -> List[Pattern]:\n",
    "    return [re.compile(p, flags) for p in patterns]\n",
    "\n",
    "AJSRP_BLOCK_RX: List[Pattern] = compile_patterns(AJSRP_BLOCK_PATTERNS, re.IGNORECASE | re.DOTALL)\n",
    "AJSRP_LINE_RX: List[Pattern]  = compile_patterns(AJSRP_LINE_PATTERNS,  re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "def ajsrp_strip_boilerplate(clean_text: str) -> str:\n",
    "    \"\"\"Assumes text already normalized by _join_pages / _basic_cleanup pipeline.\"\"\"\n",
    "    text = clean_text\n",
    "    for rx in AJSRP_BLOCK_RX:\n",
    "        text = rx.sub(\"\", text)\n",
    "    for rx in AJSRP_LINE_RX:\n",
    "        text = rx.sub(\"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) CUES + REGEXES\n",
    "# ============================================================\n",
    "\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
    "URL_RE   = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "\n",
    "# High-precision headers (AJSRP shows both English and Arabic versions)\n",
    "ABSTRACT_HDR_AR = [\"المستخلص\", \"الملخص\", \"ملخص\", \"الخلاصة\"]\n",
    "ABSTRACT_HDR_EN = [\"abstract\"]\n",
    "\n",
    "KEYWORDS_HDR_AR = [\"الكلمات المفتاحية\", \"كلمات مفتاحية\", \"الكلمات الدالة\", \"كلمات دالة\"]\n",
    "KEYWORDS_HDR_EN = [\"keywords\", \"key words\", \"index terms\"]\n",
    "\n",
    "# Often present after keywords: INTRODUCTION / المقدمة\n",
    "INTRO_HDR_AR = [\"المقدمة\", \"مقدمة\"]\n",
    "INTRO_HDR_EN = [\"introduction\"]\n",
    "\n",
    "# Author markers that appear in AJSRP\n",
    "# (not always present, but harmless)\n",
    "AUTHOR_MARKERS_AR = [\"أ.\", \"د.\", \"دكتور\", \"المؤلف\", \"المؤلفون\"]\n",
    "AUTHOR_MARKERS_EN = [\"ms.\", \"mr.\", \"dr.\", \"prof.\", \"author\", \"authors\", \"eng.\"]\n",
    "\n",
    "# Fallback author cues (affiliations)\n",
    "AR_AUTHOR_CUES = [\"جامعة\", \"كلية\", \"قسم\", \"وزارة\", \"المملكة\", \"السودان\", \"اليمن\"]\n",
    "EN_AUTHOR_CUES = [\"university\", \"faculty\", \"department\", \"ministry\", \"ksa\", \"yemen\", \"sudan\"]\n",
    "\n",
    "# Regex safety\n",
    "ABSTRACT_RX = re.compile(r\"^\\s*abstract\\s*[:：]?\\s*$\", re.IGNORECASE)\n",
    "KEYWORDS_RX = re.compile(r\"^\\s*keywords?\\s*[:：]?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "def _prep_cues(*lists: List[str]) -> List[str]:\n",
    "    \"\"\"Lowercase + remove duplicates while preserving order.\"\"\"\n",
    "    seen = set()\n",
    "    out: List[str] = []\n",
    "    for L in lists:\n",
    "        for x in L:\n",
    "            k = x.lower()\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                out.append(k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _slice_lines(lines: List[str], start: int, end: int) -> str:\n",
    "    start = max(0, start)\n",
    "    end = min(len(lines), end)\n",
    "    if start >= end:\n",
    "        return \"\"\n",
    "    return \"\\n\".join(lines[start:end]).strip()\n",
    "\n",
    "\n",
    "def _region_from_indices(lines: List[str], idxs: List[int], radius: int = 2) -> str:\n",
    "    if not idxs:\n",
    "        return \"\"\n",
    "    keep = set()\n",
    "    for i in idxs:\n",
    "        for j in range(max(0, i - radius), min(len(lines), i + radius + 1)):\n",
    "            keep.add(j)\n",
    "    return \"\\n\".join(lines[i] for i in sorted(keep)).strip()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) AJSRP PINPOINTER (DETERMINISTIC)\n",
    "# ============================================================\n",
    "\n",
    "def AJSRP_pinpoint_imp(page1: str, page2: str, max_chars: int = 9000) -> str:\n",
    "    \"\"\"\n",
    "    Deterministic AJSRP pinpointer:\n",
    "      - join+clean (via _join_pages)\n",
    "      - strip AJSRP boilerplate\n",
    "      - single pass over lines to detect anchors:\n",
    "          abstract / keywords / intro / emails\n",
    "      - slice into labeled regions for the LLM\n",
    "    \"\"\"\n",
    "    text = _join_pages(page1, page2)\n",
    "    text = ajsrp_strip_boilerplate(text)\n",
    "\n",
    "    lines = [ln.strip() for ln in text.split(\"\\n\") if ln.strip()]\n",
    "\n",
    "    abs_cues    = _prep_cues(ABSTRACT_HDR_AR, ABSTRACT_HDR_EN)\n",
    "    key_cues    = _prep_cues(KEYWORDS_HDR_AR, KEYWORDS_HDR_EN)\n",
    "    intro_cues  = _prep_cues(INTRO_HDR_AR, INTRO_HDR_EN)\n",
    "    author_mark = _prep_cues(AUTHOR_MARKERS_AR, AUTHOR_MARKERS_EN)\n",
    "    author_cues = _prep_cues(AR_AUTHOR_CUES, EN_AUTHOR_CUES)\n",
    "\n",
    "    abs_idx: Optional[int] = None\n",
    "    key_idx: Optional[int] = None\n",
    "    intro_idx: Optional[int] = None\n",
    "\n",
    "    email_idxs: List[int] = []\n",
    "    authorhint_idxs: List[int] = []  # helps slice authors even if no marker\n",
    "\n",
    "    # ---- Single pass ----\n",
    "    for i, ln in enumerate(lines):\n",
    "        lo = ln.lower()\n",
    "\n",
    "        # Abstract header: startswith cue OR matches \"ABSTRACT:\" style line\n",
    "        if abs_idx is None:\n",
    "            if any(lo.startswith(c) for c in abs_cues) or ABSTRACT_RX.match(ln):\n",
    "                abs_idx = i\n",
    "\n",
    "        # Keywords header\n",
    "        if key_idx is None:\n",
    "            if any(lo.startswith(c) for c in key_cues) or KEYWORDS_RX.match(ln):\n",
    "                key_idx = i\n",
    "\n",
    "        # Intro header (stop marker)\n",
    "        if intro_idx is None and any(lo.startswith(c) for c in intro_cues):\n",
    "            intro_idx = i\n",
    "\n",
    "        # Emails\n",
    "        if EMAIL_RE.search(ln):\n",
    "            email_idxs.append(i)\n",
    "\n",
    "        # Author hint lines near the top (names + affiliations)\n",
    "        # AJSRP tends to have: Names -> Affiliation line (University | Country)\n",
    "        if i <= 60:\n",
    "            if any(m in lo for m in author_mark) or any(c in lo for c in author_cues):\n",
    "                authorhint_idxs.append(i)\n",
    "\n",
    "    # ABSTRACT_REGION: end priority = keywords > intro > cap\n",
    "    abstract_region = \"\"\n",
    "    if abs_idx is not None:\n",
    "        if key_idx is not None and key_idx > abs_idx:\n",
    "            end = key_idx\n",
    "        elif intro_idx is not None and intro_idx > abs_idx:\n",
    "            end = intro_idx\n",
    "        else:\n",
    "            end = min(len(lines), abs_idx + 80)  # AJSRP abstracts can be long\n",
    "        abstract_region = _slice_lines(lines, abs_idx, end)\n",
    "\n",
    "    # KEYWORDS_REGION: keywords line + next line(s)\n",
    "    keywords_region = \"\"\n",
    "    if key_idx is not None:\n",
    "        keywords_region = _slice_lines(lines, key_idx, min(len(lines), key_idx + 3))\n",
    "\n",
    "    # AUTHORS_REGION:\n",
    "    # Priority: emails region > author hints in top zone\n",
    "    if email_idxs:\n",
    "        authors_region = _region_from_indices(lines, email_idxs, radius=3)\n",
    "    else:\n",
    "        # Take a tight top-zone window around first author hint\n",
    "        if authorhint_idxs:\n",
    "            start = max(0, authorhint_idxs[0] - 2)\n",
    "            authors_region = _slice_lines(lines, start, min(len(lines), start + 18))\n",
    "        else:\n",
    "            authors_region = \"\"\n",
    "\n",
    "    # TITLE_REGION:\n",
    "    # Stop at earliest of: first author hint, email, abstract\n",
    "    cut_candidates: List[int] = []\n",
    "    if authorhint_idxs:\n",
    "        cut_candidates.append(authorhint_idxs[0])\n",
    "    if email_idxs:\n",
    "        cut_candidates.append(email_idxs[0])\n",
    "    if abs_idx is not None:\n",
    "        cut_candidates.append(abs_idx)\n",
    "\n",
    "    cutoff = min(cut_candidates) if cut_candidates else min(len(lines), 30)\n",
    "\n",
    "    title_region_lines = lines[:cutoff]\n",
    "    title_region_lines = [ln for ln in title_region_lines if not URL_RE.search(ln)]\n",
    "    title_region = \"\\n\".join(title_region_lines[:40]).strip()\n",
    "\n",
    "    llm_text = (\n",
    "        \"TITLE_REGION:\\n\" + title_region +\n",
    "        \"\\n\\nAUTHORS_REGION:\\n\" + authors_region +\n",
    "        \"\\n\\nABSTRACT_REGION:\\n\" + abstract_region +\n",
    "        \"\\n\\nKEYWORDS_REGION:\\n\" + keywords_region\n",
    "    ).strip()\n",
    "\n",
    "    return llm_text[:max_chars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c65fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def AJP_pinpoint(page1: str, page2: str) -> str:\n",
    "    return AJP_pinpoint_imp(page1, page2)\n",
    "\n",
    "def AJSP_pinpoint(page1: str, page2: str) -> str:\n",
    "\n",
    "    return AJSP_pinpoint_imp(page1, page2)\n",
    "\n",
    "def ajsrp_pinpoint(page1: str, page2: str) -> str:\n",
    "    # page 2 is unnecessary for AJSRP\n",
    "    return AJSRP_pinpoint_imp(page1, \"\")\n",
    "\n",
    "def AM_pinpoint(page1: str, page2: str) -> str:\n",
    "    # TODO: tailor to AM tendencies\n",
    "    return default_pinpoint(page1, page2)\n",
    "\n",
    "def ARPD_pinpoint(page1: str, page2: str) -> str:\n",
    "    # TODO: tailor to ARPD tendencies\n",
    "    return default_pinpoint(page1, page2)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Dispatcher: pinpoint()\n",
    "# =========================\n",
    "\n",
    "_PINPOINTERS: Dict[str, Callable[[str, str], str]] = {\n",
    "    \"AJP\": AJP_pinpoint,\n",
    "    \"AJSP\": AJSP_pinpoint,\n",
    "    \"ajsrp\": ajsrp_pinpoint,\n",
    "    \"AM\": AM_pinpoint,\n",
    "    \"ARPD\": ARPD_pinpoint,\n",
    "}\n",
    "\n",
    "def pinpoint(page1: str, page2: str, source: str) -> str:\n",
    "    \"\"\"\n",
    "    Main entry point.\n",
    "    Routes to a source-specific pinpointer based on `source`.\n",
    "    If source is unknown, uses default_pinpoint.\n",
    "    \"\"\"\n",
    "    src = (source or \"\").strip()\n",
    "    fn = _PINPOINTERS.get(src, None)\n",
    "    if fn is None:\n",
    "        return None\n",
    "    return fn(page1, page2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de42a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "المجلة المصرية للتربية العلمية المجلد الخامس والعشرون العدد الرابع أكتوبر 77١٠م‏\n",
      "\n",
      "أنشطة تعلم منظم ذاتياً ‎(SRLA)‏ قائمة على\n",
      "دورة الاستقصاء التعاونى ‎(CIC)‏ لتنمية كفاءات التعلم\n",
      "الأعمق ‎(DLC)‏ والاستمتاع بتعلم العلوم لدى طلاب\n",
      "المرحلة الإعدادية\n",
      "\n",
      "إعداد\n",
      "\n",
      "أ.م.د. رباب أحمد محمد أبو الوفا\n",
      "أستاذ المناهج وطرق تدريس العلوم المساعد\n",
      "كلية التربية جامعة دمنهور\n",
      "‎Dr_rabababoelwafa @ edu.dmu.edu.eg‏\n",
      "\n",
      "رقم الإيداع: * وي ‎E. ISSN: 2735-4245 ISSN: 2536 —914 ١‏\n",
      "المجلة معرفة علي دوريات بنك المعرفة المصرىء و ‎Edu Search‏ دار المنظومة\n",
      "المجلة المصرية للتربية العلمية المجلد الخامس والعشرون العدد الرابع أكتوبر 77١٠م‏\n",
      "\n",
      "أنشطة تعلم منظم ذاتياً ‎(GRA)‏ قائمة على دورة الاستقصاء التعاونى\n",
      "‎(CIC)‏ لتنمية كفاءات التعلم الأعمق ‎(DLC)‏ والاستمتاع بتعلم العلوم\n",
      "لدى طلاب المرحلة الإعدادية\n",
      "أ.م.د/ رباب أحمد محمد أبوالوفا *\n",
      "\n",
      "المستخلص:\n",
      "\n",
      "هدف هذا البحث إلى تصميم أنشطة تعلم منظم ذاتياً قائمة على دورة\n",
      "الاستقصاء التعاونى وبحث فاعليتها فى تنمية كفاءات التعلم الأعمق والاستمتاع\n",
      "بتعلم العلوم لدى طلاب الصف الثانى الإعدادى. ‎Create‏ الأنشطة لوحدة دورية\n",
      "العناصر وخواصها وأعدت ‎Gl sal‏ جمع البيانات» وهى: اختبار إتقان مفاهيم دورية\n",
      "العناصر وخواصهاء ومقياس العقلية الأكاديمية» ومقياس الاستمتاع بتعلم العلوم.\n",
      "وقد اشتملت العينة على (121) طالبا وطالبة بالفصل الأول من العام الدراسى\n",
      "\n",
      ")2020/2021( ؤزعت عشوائياً على مجموعتين: تجريبية وعددها )61(\n",
      "وضابطة وعددها (60). طبقت أدوات جمع البيانات قبلياًء ثم تم التدريسٍ بالأنشطة\n",
      "‎Cah‏ أدوات جمع البيانات بعدياء وقد أسفرت النتائج عن وجودا فرق دال إحصائياً\n",
      "\n",
      "عند مستوى دلالة ‎p<0.01‏ بين متوسطى درجات طلاب المجموعتين فى كل من:\n",
      "العلوم لصالح المجموعة التجريبية. وؤجد علاقة ارتباطية دالة ‎thes)‏ عند\n",
      "مستوى دلالة ‎p<0.01‏ بين إتقان المفاهيم» والعقلية الأكاديمية» والاستمتاع بتعلم العلوم.\n",
      "\n",
      "الكلمات المفتاحية: أنشطة تعلم منظم ذاتياً - دورة الاستقصاء التعاونى - كفاءات\n",
      "\n",
      "التعلم الأعمق - الاستمتاع بتعلم العلوم - طلاب ‎Als yall‏ الإعدادية.\n",
      "\n",
      "* أستاذ المناهج وطرق التدريس العلوم المساعد - كلية التربية - جامعة دمنهور.\n",
      "\n",
      "رقم الإيداع: ‎E. ISSN: 2735-4245 ISSN: 2536 — 914 YErry‏\n",
      "المجلة معرفة علي دوريات بنك المعرفة المصرىء و ‎Edu Search‏ دار المنظومة\n",
      "\n",
      "دن\n"
     ]
    }
   ],
   "source": [
    "df_am = df[df[\"source\"] == \"ARPD\"]\n",
    "print ()\n",
    "print (df_am.loc[df_am.index[2], \"page1\"])\n",
    "print(df_am.loc[df_am.index[2], \"page2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3934207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1187\n",
      "1436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (count_tokens(AJSP_pinpoint_imp(sample_ajsp, \"\")))\n",
    "print (count_tokens(sample_ajsp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb0b92",
   "metadata": {},
   "source": [
    "# executing final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdcbe7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title_ar\": \"المشكلات السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس من وجهة نظر المعلمين\",\n",
      "  \"title_en\": \"Behavioral Problems of Students with Specific Learning Disabilities in Primary Schools in Jerusalem from the Perspective of Teachers\",\n",
      "  \"abstract_ar\": \"هدفت الدراسة التعرف إلى المشكلات السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس من وجهة نظر المعلمين، والتعرف إلى دور المتغيرات الجنس، وسنوات الخبرة، والمؤهل العلمي في استجابات المعلمين نحو المشكلات السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس. وقد تكون مجتمع الدراسة من جميع معلمي المدارس الأساسية في مدينة القدس والبالغ عددهم (1761)، وتكونت عينة الدراسة من (119) معلماً ومعلمة اختيرت بالطريقة الغرضية الهادفة. ولتحقيق أهداف الدراسة استخدمت الباحثة المنهج الوصفي بصورته التحليلية، واستخدام الاستبانة كأداة للدراسة. ويعد تحليل النتائج باستخدام برنامج الرزم الإحصائية للعلوم الاجتماعية (SPSS) أظهرت النتائج أن المتوسطات الحسابية والانحرافات المعيارية للمشكلات السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس من وجهة نظر المعلمين تراوحت ما بين (3.61-2.88) إذ كانت النسبة المئوية للاستجابة لها تتراوح ما بين )%57.6-%72.2(، فقد أتى المجال الأول السلوك المتعلق بالنشاط الزائد في الترتيب الأول ويمتوسط حسابي مقداره (3.61) ونسبة مئوية )%72.2( وهي درجة كبيرة، وفي المرتبة الثانية المجال الرابع وبمتوسط حسابي مقداره )3.35( ونسبة مئوية )%67-0(، وقد أتى المجال الثاني السلوك الاجتماعي المنحرف في المرتبة الثالثة ويمتوسط حسابي (3.14) وبنسبة مئوية )%62.8(، وفي المرتبة الأخيرة المجال الثالث والمتعلق بالعادات الغريبة واللزمات العصبية وبمتوسط حسابي )2-88( ونسبة مئوية )%57.6(، الدرجة الكلية فقد حصلت على متوسط حسابي )3.26( ونسبة مئوية (7665.2) وهي درجة متوسطة. تظهر فروق دالة إحصائياً في استجابات المعلمين نحو المشكلات السلوكية لدى طلبة ذوي صعوبات التعلم المحددة في المدارس الاساسية داخل القدس تعزى لمتغيري الجنس والمؤهل العلمي، ظهرت فروق في متغير سنوات الخدمة ولصالح أكثر من )10( سنة وأوصت الدراسة بتفعيل دور الإرشاد التربوي في المدارس وعقد دورات تدريبية للمعلمين الجدد حول مهارات التعامل مع المشكلات الصفية ومنها المشكلات السلوكية لذوي صعوبات التعلم المحددة.\",\n",
      "  \"abstract_en\": \"The study aimed to identify the behavioral problems of students with specific learning disabilities in primary schools in Jerusalem from the perspective of teachers, and to identify the role of gender, years of experience, and educational qualifications in teachers' responses to behavioral problems of students with specific learning disabilities in primary schools in Jerusalem. The study population consisted of all teachers in primary schools in Jerusalem (1761), and the sample consisted of (119) teachers and female teachers selected purposefully. To achieve the study objectives, the researcher used a descriptive approach in its analytical form, and the questionnaire was used as a research tool. The results of the analysis using the Statistical Package for the Social Sciences (SPSS) showed that the arithmetic means and standard deviations of the behavioral problems of students with specific learning disabilities in primary schools in Jerusalem from the perspective of teachers ranged between (3.61-2.88), with a percentage of response ranging between )%57.6-%72.2(. The first domain, which is related to excessive activity, was ranked first with an arithmetic mean of (3.61) and a percentage of )%72.2(. The fourth domain was ranked second with an arithmetic mean of )3.35( and a percentage of )%67-0(. The second domain, which is related to social deviation, was ranked third with an arithmetic mean of (3.14) and a percentage of )%62.8(. The third domain, which is related to strange habits and neurological needs, was ranked last with an arithmetic mean of )2-88( and a percentage of )%57.6(. The overall grade obtained an arithmetic mean of )3.26( and a percentage of (7665.2), which is an average grade. There were statistically significant differences in teachers' responses to behavioral problems of students with specific learning disabilities in primary schools in Jerusalem, attributed to gender and educational qualifications variables. There were differences in the years of service variable in favor of those with more than )10( years. The study recommended activating the role of educational counseling in schools and organizing training sessions for new teachers on dealing with classroom problems, including behavioral problems of students with specific learning disabilities.\",\n",
      "  \"general_field\": \"Education\",\n",
      "  \"authors\": [\n",
      "    \"أ. نفيسة سعيدة\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meta = extract_article_metadata(AJP_pinpoint_imp(sample_ajsp, \"\"))\n",
    "print(json.dumps(meta.model_dump(), ensure_ascii=False, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366fa3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85802c77-4be3-4db8-a3c9-b38479b38f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
