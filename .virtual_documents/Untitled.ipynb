


content=""
with open("ex1.txt", "r", encoding="utf-8") as file:
    content = file.read()
    print(content)





import re # step 2 3 4 
import regex # step 1 2
import emoji # step 2

from pyarabic.araby import strip_tashkeel, strip_tatweel  # step5


import nltk # step 6 

# Download tokenizer models
nltk.download('punkt')

from nltk.stem.isri import ISRIStemmer # step 6 
from nltk.tokenize import word_tokenize # step 6 

import arabicstopwords.arabicstopwords as stp #step 7





step1 = regex.sub(r'[\p{Latin}\d]+', '', content)






def clean_text(text):

    text = emoji.replace_emoji(text, replace='')

    #Remove all punctuation (any Unicode punctuation)
    text = regex.sub(r'[\p{P}\p{S}]+', '', text)


    invisible_chars = r'[\u200e\u200f\u200b\u200c\u200d\u2060]'
    text = regex.sub(invisible_chars, '', text)

    return text

# Usage
step2 = clean_text(step1)


step2





# Replace multiple spaces, tabs, newlines, and other whitespace with a single space
step3 = re.sub(r'\s+', ' ', step2)

# Trim leading and trailing spaces
step3 = step3.strip()



step3


# check point save to text 
with open("steps/step3.txt", "w", encoding="utf-8") as f:
    f.write(step3)





step4 = strip_tashkeel(step3)
step4 = strip_tatweel(step4)






def normalize_arabic(text):
    """
    Normalize Arabic characters:
    - Alef variations (أ، إ، آ) → ا
    - Yaa variations (ى, ئ) → ي
    - Taa Marbuta (ة) → ه (Haa Marbuta)
    """
    # Normalize Alef
    text = text.replace('أ', 'ا').replace('إ', 'ا').replace('آ', 'ا')
    
    # Normalize Yaa
    text = text.replace('ى', 'ي').replace('ئ', 'ي')
    
    # Normalize Taa Marbuta
    text = text.replace('ة', 'ه')
    
    return text



step5 = normalize_arabic(step4)


step5





def tokenize_and_stem(text):
    # Tokenize text into words
    tokens = word_tokenize(text)

    # Initialize Arabic stemmer
    stemmer = ISRIStemmer()

    # Stem each token
    stemmed_tokens = [stemmer.stem(token) for token in tokens]

    return stemmed_tokens


step6 = tokenize_and_stem(step5)


step6






def remove_stopwords(tokens):
    """
    Remove Arabic stopwords from a list of tokens
    """
    stopwords = set(stp.stopwords_list())
    filtered_tokens = [token for token in tokens if token not in stopwords]
    return filtered_tokens


step7 = remove_stopwords(step6)


step7


with open("steps/step7_tokens.txt", "w", encoding="utf-8") as f:
    for token in step7:
        f.write(token + "\n")



